# 漫畫自動上色專案報告

## 1. 資料集 (Dataset)

*   **處理流程**：
    1.  讀取黑白與彩色圖像對。
    2.  統一調整大小 (例如 512x512)。
    3.  彩色圖轉 LAB 色彩空間，分離 L 與 AB 通道。
    4.  L 通道 (輸入) 標準化至 `[0, 1]`。
    5.  AB 通道 (目標) 標準化至 `[-1, 1]`。
*   **核心轉換方法**：
    *   **RGB 到 LAB**：利用 LAB 空間將亮度 (L) 與顏色 (A, B) 分離的特性，簡化模型學習任務。L 通道來自輸入，模型預測 AB 通道。
        *   數學：涉及 RGB->XYZ->LAB 的標準轉換公式。
    *   **標準化**：將像素值縮放到特定範圍 (`[0,1]` 或 `[-1,1]`)，有助於穩定和加速訓練。
*   **輸入圖像的旅程 (預測時)**：
    1.  讀取黑白圖，轉灰階，調整大小。
    2.  L 通道標準化至 `[0, 1]`。
    3.  生成嵌入向量 (例如隨機噪聲)。
    4.  L 通道和嵌入向量送入模型，預測 AB 通道 (範圍 `[-1, 1]`)。
    5.  AB 通道反標準化。
    6.  原始 L 通道與預測的 AB 通道合併成 LAB 圖像。
    7.  LAB 圖像轉回 RGB 圖像。
*   **工具/框架**：
    *   `Python`: 主要語言。
    *   `TensorFlow/Keras`: 深度學習模型構建與訓練。
    *   `NumPy`: 數值與陣列操作。
    *   `Pillow (PIL)`, `OpenCV (cv2)`: 圖像讀寫、處理、色彩空間轉換。
    *   `scikit-image`: 計算 PSNR, SSIM 指標。
    *   `Matplotlib`: 繪î製結果圖與指標比較圖。
*   **訓練平台與套件**：
    *   平台：標準 Python 環境，通常在配備 NVIDIA GPU 的 Linux 伺服器/本地機器上訓練。
    *   主要套件：`tensorflow`, `numpy`, `opencv-python`, `scikit-image`, `matplotlib`, `Pillow`。

## 2. 主要方法 (Main Approach)

*   **U-Net 架構**：因其編碼器-解碼器結構和跳躍連接，能有效結合圖像的上下文與細節特徵，適合圖像到圖像轉換任務，特別是保留線稿結構的上色。
*   **LAB 色彩空間**：將亮度(L)和顏色(A,B)分離，模型只需從L預測AB，簡化任務且顏色變化更符合人類感知。
*   **GAN (可選)**：引入判別器與生成器對抗，旨在提升生成圖像的真實感、鮮豔度和清晰度，克服單純像素損失 (如MAE) 可能導致的模糊或過於平滑問題。
*   **嵌入向量**：作為額外條件輸入，隨機噪聲嵌入可能引入輸出多樣性或起正則化作用。

## 3. 評估指標 (Evaluation Metric)

*   **評估方式**：
    *   **定量指標**：PSNR (峰值信噪比) 和 SSIM (結構相似性指數)，計算預測圖像與真實圖像的數學差異。
    *   **定性指標**：視覺比較，人工檢查上色結果的自然度、美觀性和瑕疵。
*   **PSNR & SSIM**：
    *   **PSNR**：基於均方誤差 (MSE)，值越高代表失真越小，與像素級誤差敏感。
    *   **SSIM**：從亮度、對比度、結構三方面比較圖像相似性，值越接近1代表結構越相似，更能反映人類視覺感知。
*   **訓練損失函數 (非評估指標)**：
    *   **U-Net (非GAN)**: 主要使用 **MAE (L1損失)**。相較MSE(L2損失)，L1損失傾向產生更清晰、模糊更少的結果。
    *   **GAN**: 
        *   **生成器損失**: 對抗性損失 (欺騙判別器) + L1損失 (確保顏色準確性)。
        *   **判別器損失**: 區分真假圖像的二元交叉熵損失。
*   **已運行的實驗類型 (基於 `run_comparison_experiment.py` 配置)**：
    *   **模型架構**: PReLU U-Net, LeakyReLU U-Net, VGG16 U-Net。
    *   **訓練模式**: 純U-Net (MAE損失), GAN (U-Net作生成器)。
    *   **超參數**: 不同數量的 Epochs (如10, 200)，不同的 Batch Size (U-Net: 2, 4; GAN: 1, 2)，不同的 Learning Rates (U-Net: 0.0001; GAN G/D: 0.0002)，L1 Lambda (GAN中: 100)。
    *   **嵌入輸入**: 評估/預測時使用隨機噪聲向量；`unet_relu_leaky` 模型有 `use_zero_embedding` 選項。
    *   **資料集**: 從小規模 (100張) 到較大規模 (1000張) 的訓練數據。
*   **結果討論 (預期)**：
    *   GAN 預期視覺效果更生動，但訓練不穩定，PSNR/SSIM 可能略低於純U-Net。
    *   純U-Net (MAE) 結果可能較平滑，但顏色"安全"。
    *   VGG16-U-Net 可能因遷移學習在特徵提取上有優勢，但需小心調優。
    *   超參數對結果影響顯著，需實驗找到最優組合。
    *   **視覺檢查始終是最終標準**，指標僅供參考。
*   **工作局限性**：
    *   高度依賴訓練數據的質與量。
    *   對未知風格泛化能力有限。
    *   精細細節處理可能不足。
    *   缺乏真正語義理解，可能產生不合邏輯顏色。
    *   GAN 可能產生偽影。
    *   計算成本高。
    *   客觀評估指標 (PSNR/SSIM) 不能完全代表人眼主觀感受。
    *   顏色和風格可控性低。

--- 