# 詳細報告：漫畫自動上色專案

## 1. 資料集 (Dataset)

### 資料集是如何被處理的？

我們的漫畫上色專案使用成對的圖像資料：一張是黑白（灰階）的線稿或L通道圖像，另一張是其對應的彩色原圖。處理流程如下：

1.  **讀取圖像**：
    *   使用 Python 的 Pillow (PIL) 函式庫或 OpenCV (cv2) 讀取黑白圖像和彩色圖像。

2.  **調整圖像大小 (Resizing)**：
    *   將所有圖像（黑白與彩色）統一調整到模型期望的輸入尺寸，例如 512x512 像素或 256x256 像素。這確保了模型輸入的一致性。

3.  **色彩空間轉換與通道分離 (Color Space Conversion & Channel Separation)**：
    *   **對於彩色圖像**：
        1.  將 RGB 色彩空間的圖像轉換為 LAB 色彩空間。LAB 色彩空間包含三個通道：
            *   **L 通道**：代表亮度 (Lightness)。
            *   **A 通道**：代表綠色到紅色的色譜。
            *   **B 通道**：代表藍色到黃色的色譜。
        2.  分離出 L 通道和 AB 通道。
    *   **對於黑白圖像**：
        1.  如果輸入的直接是灰階圖像，它本身就可以被視為 L 通道。

4.  **數據標準化 (Normalization)**：
    *   **L 通道 (模型輸入)**：將 L 通道的像素值從原始範圍 (通常是 0-255) 標準化到 `[0, 1]` 之間。這是通過將每個像素值除以 255 來實現的。
        *   數學式：\(L_{norm} = L_{orig} / 255.0\)
    *   **AB 通道 (模型預測目標/真實值)**：將 AB 通道的像素值從原始範圍 (在 OpenCV 從 LAB 轉換後，AB 通道值通常在 0-255 範圍內，代表了原始 LAB 的約 -128 到 +127 的範圍) 標準化到 `[-1, 1]` 之間。這是通過先將像素值除以 128.0，然後減去 1.0 (或者等效地，減去128再除以128) 來實現的。
        *   數學式：\(AB_{norm} = (AB_{0\_255} / 128.0) - 1.0\)  或 \(AB_{norm} = (AB_{0\_255} - 128.0) / 128.0\)

5.  **模型輸入準備**：
    *   標準化後的 L 通道 (`[0, 1]`) 作為模型的一個主要輸入。
    *   一個嵌入向量 (embedding vector) 作為模型的另一個輸入。這個向量可以是：
        *   **全零向量**：如果模型的 `use_zero_embedding` 參數設置為 True。
        *   **隨機噪聲向量**：從標準正態分佈中採樣得到 (`np.random.randn()`)。這在訓練和評估時用於引入一定的隨機性或讓模型學習一個潛在的風格空間。

### 用什麼方法將圖像轉換成我們理想的形式？背後的數學原理是什麼？

**主要方法：**

1.  **色彩空間轉換 (RGB to LAB)**：
    *   **為何選擇 LAB？** LAB 色彩空間被設計為更接近人類的視覺感知。它將圖像的亮度資訊 (L) 與色彩資訊 (A, B) 分離。這樣做的好處是：
        *   模型可以專注於學習如何根據亮度來預測顏色，簡化了學習任務。
        *   在 LAB 空間中計算損失函數，其變化更能反映人類感知的顏色差異。
    *   **數學原理**：
        *   RGB 到 XYZ：首先，RGB 值通過一個線性變換矩陣轉換為 XYZ 色彩空間。
            \[
            \begin{bmatrix} X \\ Y \\ Z \end{bmatrix} = M \begin{bmatrix} R \\ G \\ B \end{bmatrix}
            \]
            矩陣 \(M\) 取決於 RGB 的具體色域標準（例如 sRGB）。
        *   XYZ 到 LAB：然後，XYZ 值通過非線性變換轉換為 LAB。 \(L^*\) 的計算與 \(Y\)（亮度）相關，而 \(a^*\) 和 \(b^*\) 的計算涉及到 \(X, Y, Z\) 之間的差異。
            \[ L^* = 116 f(Y/Y_n) - 16 \]
            \[ a^* = 500 [f(X/X_n) - f(Y/Y_n)] \]
            \[ b^* = 200 [f(Y/Y_n) - f(Z/Z_n)] \]
            其中 \(X_n, Y_n, Z_n\) 是參考白點的 CIE XYZ 三刺激值。函數 \(f(t)\) 定義為：
            \[ f(t) = t^{1/3} \quad \text{if } t > (6/29)^3 \]
            \[ f(t) = (1/3)(29/6)^2 t + 4/29 \quad \text{otherwise} \]
            OpenCV 的 `cv2.cvtColor` 內部實現了這些轉換。 L 通道通常範圍是 [0, 100]，A 和 B 通道範圍大約是 [-128, 127]。OpenCV 在輸出時會將這些值映射到 [0, 255] 的範圍內，以便於圖像存儲。

2.  **標準化 (Normalization)**：
    *   **為何標準化？** 神經網路的訓練對於輸入數據的尺度很敏感。將數據標準化到一個較小的、固定的範圍（如 `[0, 1]` 或 `[-1, 1]`）有助於：
        *   加快訓練過程的收斂速度。
        *   防止梯度爆炸或消失問題。
        *   使不同特徵（在此情境下是像素值）的權重在相似的尺度上。
    *   **L 通道至 `[0, 1]`**： \(L_{norm} = L_{orig\_0\_255} / 255.0\)
    *   **AB 通道至 `[-1, 1]`**： \(AB_{norm} = (AB_{orig\_0\_255} - 128.0) / 128.0\)。這將原來 OpenCV輸出的 0-255 範圍（代表 LAB 中的約 -128 到 +127）映射到 `[-1, 1]`。

### 如果我輸入一張圖像，它會經歷哪些過程？

**訓練階段 (Training Phase):**

1.  **載入圖像對**：讀取一張黑白/L通道圖像和其對應的原始彩色圖像。
2.  **預處理 (Preprocessing)**：
    *   **調整大小**：兩張圖像都被 resize 到模型指定的尺寸 (例如 512x512)。
    *   **色彩轉換**：彩色圖像從 RGB 轉換到 LAB。
    *   **通道分離**：從 LAB 彩色圖像中分離出 L 通道和 AB 通道。
    *   **標準化**：
        *   輸入的 L 通道 (來自黑白圖或彩色圖的L) 標準化到 `[0, 1]`。
        *   目標的 AB 通道標準化到 `[-1, 1]`。
3.  **準備嵌入向量**：生成一個嵌入向量 (全零或隨機噪聲)。
4.  **模型輸入**：將標準化後的 L 通道和嵌入向量一同送入 U-Net 模型 (生成器)。
5.  **模型輸出**：模型預測出 AB 通道 (仍在 `[-1, 1]` 範圍內)。
6.  **計算損失**：
    *   **對於純 U-Net 訓練**：計算預測的 AB 通道與真實的 AB 通道之間的損失 (例如 MAE - 平均絕對誤差)。
    *   **對於 GAN 訓練**：
        *   生成器損失：包括對抗性損失 (判別器對生成圖像的判斷) 和 L1 損失 (預測 AB 與真實 AB 的差異)。
        *   判別器損失：判別器區分真實圖像和生成圖像的能力。
7.  **反向傳播與優化**：根據損失更新模型的權重。

**預測/上色階段 (Inference/Colorization Phase - 例如 `predict.py` 中的 `colorize_folder`):**

1.  **載入輸入圖像**：讀取一張黑白 (灰階) 圖像。
2.  **預處理 (Preprocessing)**：
    *   **調整大小**：Resize 到模型訓練時使用的尺寸 (例如 512x512)。
    *   **轉換為 L 通道**：確保圖像為單通道灰階。
    *   **標準化**：將 L 通道像素值標準化到 `[0, 1]`。
3.  **準備嵌入向量**：生成一個嵌入向量 (通常與訓練時使用的策略一致，例如隨機噪聲)。
4.  **模型預測**：將標準化後的 L 通道和嵌入向量送入已訓練好的 U-Net 模型，得到預測的標準化 AB 通道 (範圍 `[-1, 1]`)。
5.  **後處理 (Postprocessing)**：
    *   **反標準化 AB 通道**：將預測的 AB 通道從 `[-1, 1]` 轉換回原始 LAB 色彩空間中 AB 通道的近似範圍 (例如，通過乘以 128 再加上 128，得到 0-255 範圍)。
    *   **準備 L 通道進行重建**：使用輸入的原始灰階圖像的 L 通道 (確保其像素範圍是 LAB 空間所期望的，通常是 0-255 代表 0-100 的 L值)。`utils.py` 中的 `reconstruct_rgb_from_lab` 使用的是 0-255 的 L 通道。
    *   **合併 LAB 通道**：將原始的 L 通道與預測並反標準化後的 AB 通道合併，形成一個完整的 LAB 圖像。
    *   **轉換回 RGB**：將 LAB 圖像轉換回 RGB 色彩空間 (OpenCV 中通常是 LAB -> BGR -> RGB)。
6.  **保存/顯示結果**：保存上色後的 RGB 圖像或在螢幕上顯示。

### 我們使用了哪些工具或框架？它們的用途是什麼？

*   **Python**: 主要的程式設計語言。
*   **TensorFlow/Keras (`tensorflow.keras`)**:
    *   **用途**: 核心的深度學習框架。用於定義、編譯、訓練和評估神經網路模型，如 U-Net 和 GAN 中的生成器與判別器。
*   **NumPy**:
    *   **用途**: Python 中進行科學計算的基礎套件。主要用於高效地處理圖像數據的多維陣列，執行數值轉換和操作。
*   **Pillow (PIL Fork)**:
    *   **用途**: 圖像處理函式庫。用於讀取、寫入、調整大小、轉換不同圖像檔案格式等基本圖像操作。
*   **OpenCV (`cv2`)**:
    *   **用途**: 開源電腦視覺函式庫。在此專案中，主要用於色彩空間轉換 (例如 RGB <-> LAB) 以及可能的其他圖像處理輔助任務。
*   **scikit-image**:
    *   **用途**: 圖像處理演算法集合。用於計算圖像品質評估指標，如 PSNR (峰值信噪比) 和 SSIM (結構相似性指數)。
*   **Matplotlib**:
    *   **用途**: Python 的繪圖函式庫。用於生成結果圖像的視覺化比較圖 (輸入L、預測RGB、真實RGB)，以及繪製模型性能指標 (PSNR, SSIM) 的比較圖表。
*   **argparse**:
    *   **用途**: Python 標準函式庫，用於解析命令列參數。使得腳本 (`main.py`, `utils.py`, `predict.py`) 可以接收來自使用者的不同輸入設定，增加了靈活性。
*   **subprocess**:
    *   **用途**: Python 標準函式庫，用於創建和管理子行程。在 `run_comparison_experiment.py` 中，它被用來多次調用 `main.py` 以執行不同的實驗配置。
*   **tqdm**:
    *   **用途**: 一個快速、可擴展的 Python 進度條模組。在數據加載、模型訓練的 epoch 迭代等長時間運行的循環中顯示進度，提升用戶體驗。
*   **os, json, glob**:
    *   **用途**: Python 標準函式庫，分別用於操作系統交互 (如路徑操作、創建資料夾)、讀寫 JSON 格式的數據 (如保存評估指標) 和查找符合特定模式的檔案路徑 (如查找圖像檔案)。

### 我們在哪個訓練平台上進行？使用了哪些 Python 套件及其用途？

*   **訓練平台 (Training Platform)**:
    *   此專案的腳本設計為可以在標準的 Python 環境中運行。訓練通常在具有 NVIDIA GPU 的本地機器或遠程伺服器上進行，以加速深度學習模型的計算。TensorFlow 會自動檢測並使用可用的 GPU。從您的使用環境來看 (`vscode-remote://ssh-remote...`)，您可能是在一台遠程 Linux 伺服器上工作。

*   **Python 套件及其用途 (已在上文詳細列出)**:
    *   `tensorflow` (或 `tensorflow-gpu`): 深度學習。
    *   `numpy`: 數值計算。
    *   `Pillow`: 基礎圖像處理。
    *   `opencv-python`: 電腦視覺，色彩空間轉換。
    *   `scikit-image`: 圖像評估指標。
    *   `matplotlib`: 繪圖與視覺化。
    *   `argparse`: 命令列介面。
    *   `tqdm`: 進度條。
    *   標準函式庫如 `os`, `json`, `glob`, `subprocess`: 系統、檔案和流程管理。

## 2. 主要方法 (Main Approach)

### 為什麼選擇這樣的方法進行漫畫上色？

我們選擇的漫畫上色方法主要基於以下幾個核心組件和考量：

*   **U-Net 架構 (U-Net Architecture)**:
    *   **原因**: U-Net 是一種為生物醫學圖像分割設計的卷積神經網路架構，但它在各種圖像到圖像的轉換任務（包括圖像上色）中都表現出色。
    *   **優勢**:
        1.  **編碼器-解碼器結構 (Encoder-Decoder Structure)**: 編碼器部分逐漸減少空間維度並增加特徵通道，從而捕捉圖像的上下文資訊和抽象特徵。解碼器部分則逐步恢復空間維度，並利用編碼器提取的特徵來重建輸出圖像。
        2.  **跳躍連接 (Skip Connections)**: U-Net 的關鍵特性是其跳躍連接，它將編碼器中較早層的特徵圖直接連接到解碼器中對應的較後層。這使得網路能夠將高層次的語義特徵（來自編碼器深層）與低層次的細節特徵（來自編碼器淺層）相結合，有助於生成更精確、細節更豐富的彩色圖像。對於漫畫線稿，保留線條結構的同時填充顏色至關重要，跳躍連接對此非常有益。
        3.  **多尺度處理能力**: 由於其下採樣和上採樣路徑，U-Net 能夠有效地處理不同尺度的圖像特徵。

*   **LAB 色彩空間 (LAB Color Space)**:
    *   **原因**: 如前所述，LAB 色彩空間將亮度 (L) 與顏色 (A, B) 分離。
    *   **優勢**:
        1.  **簡化任務**: 模型只需要學習從輸入的 L 通道（黑白圖像）預測對應的 A 和 B 顏色通道。亮度資訊直接由輸入提供。
        2.  **感知一致性**: LAB 空間中的歐氏距離比 RGB 空間更能代表人眼感知的顏色差異。因此，在 LAB 空間定義損失函數（例如，比較預測的 AB 通道與真實的 AB 通道）可能引導模型產生視覺上更自然的顏色。
        3.  **避免不自然的亮度變化**: 由於 L 通道是固定的（來自輸入），模型不會意外地改變圖像的整體明暗結構，而是專注於著色。

*   **生成對抗網路 (GAN - Generative Adversarial Network) - 作為進階選項**:
    *   **原因**: 雖然僅使用 U-Net 配合 L1 或 L2 損失（如 MAE）可以直接進行上色，但這類損失函數傾向於產生所有可能顏色的平均值，導致結果有時顯得過於平滑或模糊，缺乏鮮豔度和真實感。GAN 的引入是為了改善這一點。
    *   **優勢**:
        1.  **提升真實感和清晰度**: GAN 由一個生成器 (在此專案中是 U-Net) 和一個判別器組成。生成器嘗試生成逼真的彩色圖像，而判別器則學習區分真實的彩色圖像和生成的圖像。這種對抗過程迫使生成器產生更清晰、更具說服力的細節和顏色。
        2.  **PatchGAN 判別器**: 我們可能考慮使用 PatchGAN 判別器。它不是對整張圖像的真實性進行單一評估，而是對圖像中的多個局部小塊 (patches) 進行評估。這鼓勵生成器在圖像的各個局部區域都產生逼真的紋理和細節，對於圖像轉換任務尤其有效。
        3.  **組合損失函數**: GAN 的生成器通常使用組合損失函數，包括對抗性損失（來自判別器的反饋）和傳統的像素級損失（如 L1 損失，以確保顏色與真實目標大致匹配）。這有助於平衡生成結果的真實性和準確性。

*   **嵌入向量 (Embedding Vector)**:
    *   **原因**: 引入一個額外的嵌入向量（全零或隨機噪聲）可以為模型提供一個額外的條件輸入。
    *   **潛在優勢**:
        1.  **引入隨機性/多樣性**: 如果使用隨機噪聲向量，並且模型學會利用這個輸入，它可能為相同的黑白輸入產生略有不同的著色結果，增加輸出的多樣性。
        2.  **學習風格表示 (理論上)**: 在更複雜的設定中，如果數據集包含不同風格的著色，嵌入向量有潛力被訓練來捕捉和控制這些風格。在當前的專案迭代中，它主要作為一個附加的條件輸入，其影響取決於模型如何學習利用它。隨機噪聲嵌入在訓練時有時也能起到一定的正則化作用。

**總結來說，這種方法結合了 U-Net 強大的圖像特徵提取和空間重建能力，LAB 色彩空間在感知上更優的特性，以及可選的 GAN 框架來提升生成結果的真實感和視覺效果，使其成為一個適用於漫畫上色的綜合解決方案。**

## 3. 評估指標 (Evaluation Metric)

### 我們如何評估模型？

我們使用定量指標和定性指標相結合的方式來評估模型的上色性能。

*   **定量指標 (Quantitative Metrics)**:
    *   這些指標通過數學計算來衡量預測圖像與原始真實彩色圖像之間的差異。
    1.  **PSNR (Peak Signal-to-Noise Ratio - 峰值信噪比)**
    2.  **SSIM (Structural Similarity Index Measure - 結構相似性指數)**

*   **定性指標 (Qualitative Metrics)**:
    *   這依賴於人工視覺檢查，以判斷上色結果的自然程度、美觀性、是否存在明顯的瑕疵（如顏色溢出、不自然的顏色選擇等）。
    1.  **視覺比較**: 直接觀察模型生成的彩色圖像，並與原始的彩色圖像（Ground Truth）進行對比。
    2.  **比較圖**: `utils.py` 和 `run_comparison_experiment.py` 腳本會生成包含輸入L通道圖像、模型預測的彩色圖像和原始彩色圖像的並排比較圖，方便進行細緻的視覺評估。

### SSIM 和 PSNR 是如何計算的？它們代表什麼？

*   **PSNR (Peak Signal-to-Noise Ratio - 峰值信噪比)**:
    *   **計算方式**: PSNR 是基於預測圖像和原始圖像之間的均方誤差 (MSE) 來計算的。MSE 首先計算兩張圖像對應像素值之差的平方的平均值。
        \[ MSE = \frac{1}{m \times n} \sum_{i=0}^{m-1} \sum_{j=0}^{n-1} [I(i,j) - K(i,j)]^2 \]
        其中 \(I\) 是原始圖像，\(K\) 是預測圖像，\(m \times n\) 是圖像的像素總數。
        然後，PSNR 計算如下：
        \[ PSNR = 20 \cdot \log_{10}(MAX_I) - 10 \cdot \log_{10}(MSE) \]
        其中 \(MAX_I\) 是圖像像素值的最大可能值（例如，對於 8 位元圖像，\(MAX_I = 255\)）。
    *   **代表意義**: PSNR 以分貝 (dB) 為單位。它衡量的是信號（原始圖像）的最大可能功率與影響其表示精度的雜訊（誤差）功率之間的比例。
        *   **PSNR 值越高，通常表示預測圖像的品質越好，失真越小。**
        *   PSNR 對於像素級的絕對誤差非常敏感。然而，它可能不總是與人類對圖像品質的主觀感知完全一致。例如，一個在結構上正確但整體顏色稍有偏差的圖像，其 PSNR 可能會比較低。

*   **SSIM (Structural Similarity Index Measure - 結構相似性指數)**:
    *   **計算方式**: SSIM 是一種更複雜的指標，它試圖從結構資訊的角度來衡量兩張圖像的相似性。它基於三個比較項：亮度 (luminance, \(l\))、對比度 (contrast, \(c\)) 和結構 (structure, \(s\))。
        \[ SSIM(x,y) = [l(x,y)]^\alpha \cdot [c(x,y)]^\beta \cdot [s(x,y)]^\gamma \]
        通常，\(\alpha, \beta, \gamma\) 設置為 1。
        *   亮度比較: \(l(x,y) = \frac{2\mu_x\mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1}\)
        *   對比度比較: \(c(x,y) = \frac{2\sigma_x\sigma_y + C_2}{\sigma_x^2 + \sigma_y^2 + C_2}\)
        *   結構比較: \(s(x,y) = \frac{\sigma_{xy} + C_3}{\sigma_x\sigma_y + C_3}\)
        其中 \(\mu_x, \mu_y\) 是圖像 \(x, y\) 的平均值；\(\sigma_x^2, \sigma_y^2\) 是方差；\(\sigma_{xy}\) 是協方差。\(C_1, C_2, C_3\) 是穩定常數。計算通常在圖像的局部窗口上進行，然後取平均 SSIM 值。
    *   **代表意義**: SSIM 的值域為 `[-1, 1]`（在實際應用於圖像時通常是 `[0, 1]`）。
        *   **SSIM 值越接近 1，表示兩張圖像在結構上越相似，即預測圖像的品質越好。**
        *   相較於 PSNR，SSIM 被認為更能反映人類視覺系統對圖像結構失真的感知。因此，在很多圖像生成和修復任務中，SSIM 是一個非常受歡迎的評估指標。

### 我們使用了哪種損失函數進行評估？為什麼選擇它？

需要澄清的是，損失函數主要用於**模型訓練**階段，指導模型權重的更新，而不是直接用於**評估**階段的模型性能比較。評估階段我們使用上述的 PSNR 和 SSIM。

在模型訓練中，我們使用了（或配置了使用）以下損失函數：

*   **對於標準 U-Net 模型 (非 GAN 模式)**:
    *   **MAE (Mean Absolute Error - 平均絕對誤差) / L1 損失**:
        *   **計算方式**: \(L_1 = \frac{1}{N} \sum_{i=1}^{N} |y_{true,i} - y_{pred,i}|\)，其中 \(y_{true}\) 是真實的 AB 通道值，\(y_{pred}\) 是模型預測的 AB 通道值，\(N\) 是像素總數。
        *   **選擇原因**:
            1.  **減少模糊**: 相較於 MSE (L2 損失)，L1 損失對異常值不那麼敏感，並且在圖像生成任務中，它傾向於產生更清晰、更少模糊的結果。MSE 傾向於懲罰大的錯誤，有時會導致模型選擇所有可能解的"平均"方案，從而產生模糊。
            2.  **常用選擇**: 在圖像到圖像的轉換任務中，L1 損失是一個常見且有效的選擇，用於確保生成圖像在像素級別上接近目標圖像。
        *   在 `run_comparison_experiment.py` 中，`COMMON_LOSS_TYPE` 被設置為 `"mae"`。

*   **對於 GAN 模型**:
    *   GAN 的訓練涉及到兩個損失函數：一個用於生成器 (U-Net)，一個用於判別器。
    *   **生成器損失 (Generator Loss)**:
        1.  **對抗性損失 (Adversarial Loss)**: 通常是基於判別器的輸出計算的二元交叉熵損失。目標是讓生成器生成的圖像能夠"欺騙"判別器，使其相信生成的圖像是真實的。
            數學上，生成器試圖最小化 \(\log(1 - D(G(z)))\)，其中 \(D\) 是判別器，\(G\) 是生成器，\(z\) 是輸入（L通道和嵌入向量）。
        2.  **L1 損失 (重建損失)**: 與上述 U-Net 的 L1 損失相同，計算生成器輸出的 AB 通道與真實 AB 通道之間的 MAE。這有助於確保生成的圖像不僅看起來真實，而且在內容上與輸入的 L 通道對應的顏色相符。
            生成器的總損失是這兩部分損失的加權和： \(L_G = L_{Adversarial} + \lambda \cdot L_1\)，其中 \(\lambda\) (即 `lambda_l1`) 控制 L1 損失的相對重要性。
        *   **選擇原因**: 對抗性損失驅使模型生成更銳利、更真實的圖像，而 L1 損失則確保了顏色的準確性和內容的一致性。
    *   **判別器損失 (Discriminator Loss)**:
        *   判別器的目標是準確地區分真實圖像和由生成器生成的"假"圖像。它也是基於二元交叉熵損失計算的。
            判別器試圖最小化 \( -(\log(D(x)) + \log(1 - D(G(z)))) \)，其中 \(x\) 是真實圖像。
        *   **選擇原因**: 這是 GAN 框架的標準組成部分，用於訓練判別器以提供有效的梯度信號給生成器。

### 我們之前運行過哪些類型的實驗？（例如批次大小、學習率、嵌入輸入（零向量、噪聲向量）、週期、資料集等）

根據 `run_comparison_experiment.py` 腳本的配置以及對話摘要，我們設計並可以運行以下類型的實驗：

*   **模型架構 (Model Architectures)**:
    *   `PReLU_UNet`: 使用 PReLU (Parametric ReLU) 激活函數的 U-Net 模型 (`unet_advanced_prelu`)。
    *   `LeakyReLU_UNet`: 使用 LeakyReLU 激活函數的 U-Net 模型 (`best_version` 或 `unet_relu_leaky`)。
    *   `VGG16_UNet`: 使用預訓練的 VGG16 作為編碼器部分的 U-Net 模型 (`unet_vgg16`)。

*   **訓練模式 (Training Modes)**:
    *   `unet`: 標準的 U-Net 模型訓練，僅使用像素級損失 (如 MAE)。
    *   `gan`: 生成對抗網路訓練模式，其中 U-Net 作為生成器，並額外訓練一個判別器。

*   **超參數 (Hyperparameters)**:
    *   **週期 (Epochs)**: `COMMON_EPOCHS` (例如，腳本中曾設為 10，後續用戶修改為 200)。更多的週期允許模型在數據上學習更長時間，但需注意過擬合。
    *   **批次大小 (Batch Size)**:
        *   U-Net: `COMMON_BATCH_SIZE_UNET` (例如 2，後修改為 4)。
        *   GAN: `COMMON_BATCH_SIZE_GAN` (例如 1，後修改為 2)。GAN 通常對較小的批次大小更敏感。
    *   **學習率 (Learning Rate)**:
        *   U-Net: `COMMON_LR_UNET` (例如 0.0001)。
        *   GAN 生成器 (L1 部分): 使用 `COMMON_LR_UNET`。
        *   GAN 生成器 (對抗部分): `COMMON_LR_GAN_G` (例如 0.0002)。
        *   GAN 判別器: `COMMON_LR_GAN_D` (例如 0.0002)。
    *   **損失類型 (Loss Type for U-Net / G's L1 part)**: `COMMON_LOSS_TYPE` (固定為 `"mae"`)。
    *   **GAN 的 L1 損失權重 (\(\lambda_{L1}\))**: `COMMON_LAMBDA_L1` (例如 100)。

*   **嵌入輸入 (Embed Input)**:
    *   對於 `unet_relu_leaky` 模型，有一個 `use_zero_embedding` 參數可以控制是使用全零向量還是其他（如隨機）嵌入。
    *   在 `utils.py` (評估) 和 `predict.py` (預測) 中，嵌入向量是通過 `np.random.randn()` 生成的隨機噪聲向量。這意味著使用這些腳本進行評估或預測時，模型會接收隨機嵌入。為了保持一致性，對應模型的訓練也應該使用隨機嵌入。

*   **資料集 (Dataset)**:
    *   **訓練資料**:
        *   早期/測試階段可能使用較小規模的資料集，如 "100img-paul_bw" 和 "100img-paul"。
        *   `run_comparison_experiment.py` 中後續配置為使用更大規模的資料集，如 "1000img-paul_bw" 和 "1000img-paul"。
    *   **測試資料**:
        *   L 通道輸入: `TEST_L_DIR = "test_data/l_channel"`
        *   彩色真實對照: `TEST_COLOR_DIR = "test_data/color"`

### 結果討論

由於我沒有實際運行這些實驗並獲得具體的 PSNR/SSIM 分數和視覺結果，以下討論是基於典型預期和常見觀察：

*   **不同 U-Net 變體 (PReLU vs. LeakyReLU vs. VGG16-U-Net)**:
    *   **PReLU vs. LeakyReLU**: PReLU 的負斜率是可學習的，而 LeakyReLU 是固定的。理論上 PReLU 提供了更大的靈活性，但實際性能差異可能很小，且依賴於數據集。兩者都是為了解決 ReLU 在負區間梯度為零（導致神經元"死亡"）的問題。
    *   **VGG16-U-Net**: 使用 VGG16 這樣在大型數據集 (如 ImageNet) 上預訓練的編碼器，可以帶來遷移學習的優勢。VGG16 提取的特徵可能更具通用性和魯棒性，對於我們的漫畫上色任務，如果訓練數據相對有限，這可能會有幫助。但 VGG16 也會增加模型的複雜度和參數數量。其效果還取決於如何微調預訓練的權重。
    *   **預期**: VGG16-U-Net 可能在特徵提取上更有優勢，但需要小心調整以避免過擬合，並且計算成本更高。PReLU 和 LeakyReLU U-Net 之間的差異可能不明顯，除非進行大量細緻的調優。

*   **U-Net (MAE 損失) vs. GAN**:
    *   **U-Net (MAE)**: 通常會產生顏色比較"安全"、平滑的結果。由於 MAE (L1 loss) 傾向於取所有可能解的平均，可能會導致顏色不夠鮮豔，細節處略顯模糊，但整體錯誤較少。PSNR/SSIM 分數可能相對較高。
    *   **GAN**: 預期會產生更銳利、更生動、細節更豐富的圖像。由於判別器的存在，GAN 被驅使生成看起來更"真實"的圖像。然而，GAN 的訓練更不穩定，更容易出現模式崩潰 (mode collapse，即生成器只產生非常有限的幾種輸出) 或偽影 (artifacts)。GAN 的 PSNR/SSIM 分數有時可能略低於僅用 L1 損失訓練的 U-Net，但視覺效果可能更佳。
    *   **視覺差異**: GAN 的結果可能在紋理和顏色過渡上更接近人類手繪，而 MAE 的結果可能更像"濾鏡"效果。

*   **嵌入輸入的影響 (零向量 vs. 噪聲向量)**:
    *   **零向量**: 提供一個恆定的、無資訊的輸入。模型將完全依賴於 L 通道來進行預測。
    *   **噪聲向量**: 引入隨機性。如果模型學會利用這個噪聲輸入，它可能能夠為同一個 L 通道輸入在不同的預測運行中（如果每次都重新採樣噪聲）產生略微不同的著色結果，增加輸出的多樣性。在某些情況下，噪聲輸入也可以起到一定的正則化作用。其具體影響取決於模型架構以及它如何學習利用這個額外輸入。

*   **超參數的影響**:
    *   **週期 (Epochs)**: 更多的週期通常會提高在訓練集上的性能，但也增加了過擬合的風險。需要通過驗證集監控（如果設置了的話）或觀察測試集結果的變化趨勢。
    *   **批次大小 (Batch Size)**: 較大的批次大小提供更穩定的梯度估計，但需要更多記憶體。較小的批次大小引入更多噪聲，有時有助於跳出局部最優，但也可能使訓練不穩定。GAN 通常對批次大小更敏感，較小的批次可能效果更好或更容易訓練。
    *   **學習率 (Learning Rate)**: 學習率是訓練中最關鍵的超參數之一。過高會導致訓練發散，過低會導致訓練過慢或卡在次優解。Adam 等自適應學習率優化器通常對初始學習率有較好的魯棒性。

*   **指標的解讀**:
    *   PSNR 和 SSIM 是有用的參考，但**視覺品質是最終的評判標準**。一個 GAN 模型即使 PSNR/SSIM 略低，但如果其生成的圖像在視覺上更自然、更生動，也可能被認為是更好的模型。
    *   需要仔細檢查 `visual_comparisons_gallery` 中生成的對比圖像，以全面評估不同模型和配置的優劣。

### 這項工作的局限性

1.  **資料集依賴性**: 模型的性能高度依賴於訓練數據的品質、數量和多樣性。如果訓練數據主要包含特定風格的漫畫或顏色使用模式，模型可能難以很好地泛化到其他風格。
2.  **對未知風格的泛化能力**: 漫畫的藝術風格千差萬別（線條粗細、陰影風格、上色技巧等）。在特定風格數據上訓練的模型，應用於風格差異較大的新漫畫時，效果可能會下降。
3.  **精細細節和複雜紋理的處理**: 對於極其精細的線條、複雜的背景紋理或細小的圖案，模型可能難以準確上色，有時會出現顏色溢出或細節模糊的情況，尤其是非 GAN 模型。
4.  **缺乏真正的語義理解**: 模型是通過學習 L 通道和 AB 通道之間的統計相關性來上色的，它並不真正"理解"圖像的內容（例如，"這是一片天空，所以應該是藍色"）。這可能導致在某些模糊區域或未在訓練數據中充分出現的對象上產生不自然的顏色。
5.  **顏色滲透與偽影 (Color Bleeding/Artifacts)**: 尤其是 GAN 模型，雖然能產生更銳利的結果，但如果訓練不夠充分或不穩定，有時會引入不自然的偽影或顏色滲透到不應有顏色的區域（例如線稿之外）。
6.  **計算成本**: 訓練深度 U-Net 模型，尤其是 GAN 模型，計算成本較高，通常需要 GPU 加速，並且訓練時間較長。
7.  **評估指標的局限性**: PSNR 和 SSIM 雖然是常用的客觀指標，但它們並不能完全捕捉人類對色彩和藝術美感的主觀感知。有時，指標分數高的圖像在視覺上可能並不討喜，反之亦然。
8.  **可控性有限**: 當前的模型對於輸出的具體顏色或風格提供的直接控制手段有限。雖然隨機嵌入可能帶來一些多樣性，但用戶無法像在繪圖軟件中那樣精確指定顏色。
9.  **文字和對話框的處理**: 模型可能會嘗試對漫畫中的文字或對話框內部進行上色，這通常是不希望的（它們應保持白色或特定的背景色）。這可能需要特定的數據增強策略（例如，在訓練時識別並忽略這些區域）或後處理步驟來修正。
10. **顏色一致性**: 對於系列漫畫或同一角色在不同場景下的顏色，模型可能無法保證完美的顏色一致性，除非數據集本身就強調了這一點。

## 參考工具與框架 (References - Tools & Frameworks)

*   Python
*   TensorFlow & Keras
*   NumPy
*   Pillow (PIL)
*   OpenCV (cv2)
*   Scikit-image
*   Matplotlib

--- 